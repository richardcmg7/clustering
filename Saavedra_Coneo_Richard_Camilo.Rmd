---
title: "Actividad 3 - Actividad grupal: Detección de anomalías y técnicas de agrupamiento"
date: "6/22/2020"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
<p>Grupo 10: Integrantes: </p>
<center>
<ul>
<li>Cortés Forero Leydi Milena </li>
<li>Saavedra Coneo Richard Camilo </li>
<li>Rodríguez Angarita Ramón </li>
<li>Zapata Llano Juan Sebastián </li>
</ul></center>
```{r}
#install.packages("corrplot")
#install.packages("gmodels")
#install.packages("tidyverse")
#install.packages("readr")
#install.packages("caret")
#install.packages('Hmisc')
#install.packages("modeest") 

# install.packages("e1071")
# install.packages("caTools")
# install.packages("h2o")
# install.packages("dygraphs")
# install.packages("dplyr")
# install.packages("DT")


```

```{r warning=FALSE, results='hide', message=FALSE}
library( h2o )
library(caTools)
library(e1071)
library(gmodels)
library(caret)
library(tidyverse)
library(Hmisc)
library(modeest)
library(cluster)
library(corrplot)

```

### Se cargan los datos a la variable `data`

```{r}
set.seed(1234)
data <- read_csv("data/datos.csv")


```

Se revisa si existen valores nulos.

```{r}
str(data)
```
Se hallan de las variables numéricas el valor mínimo máximo, la mediana y la media.

Se hallan de las variables categóricas las diferentes categorías y la frecuencia de cada una de ellas.

Se eliminó la variable Transaction date por tener todos los datos nulos.


```{r}
data$`Transaction date` <- NULL

```

Volver las columnas categóricas a númericas

```{r}
data$isFradulent <- as.factor(data$isFradulent)
data$isForeignTransaction <- as.factor(data$isForeignTransaction)
data$isHighRiskCountry <- as.factor(data$isHighRiskCountry)
data$`Is declined` <- as.factor(data$`Is declined`)
```
Se ejecuta la función summary para ver los valores máximos y mínimos de cada una de las variables numéricas, así como la media, se muestra también la frecuencia de las variables categóricas.

```{r}
summary(data)

```


Se cambian los valores que estaban en caracter de Y y N reeemplazandolos por 1 y 0 respectivamente

```{r}
data$isFradulent <- ifelse(data$isFradulent == "N",0,1)
data$`Is declined` <- ifelse(data$`Is declined` == "N",0,1)
data$isHighRiskCountry <- ifelse(data$isHighRiskCountry == "N",0,1)
data$isForeignTransaction<- ifelse(data$isForeignTransaction == "N",0,1)

str(data)
summary(data)
```

### Se hallan la correlaciones existentes entre las variables del conjunto de datos mediante la matriz de correlación

```{r}
corr <- cor(data[,])
corrGreater <- findCorrelation(corr, cutoff=0.8)
corrGreater
```

Se grafica la Matriz de correlación

```{r results="hide"}

Mat_Correlacion <- cor(data, method = "pearson")
round(Mat_Correlacion, digits = 2)
corrplot(Mat_Correlacion)

```

### Conjunto de datos de modelización y validación.

Se toman los datos para la modelización del 80'%' para train y él 20% para test.

```{r}
train_data <- sample(nrow(data), 0.8 * nrow(data))
data_train <- data[train_data, ]
data_test <- data[-train_data, ]

```

```{r error=FALSE, message=FALSE, results='hide'}
h2o.init()
```

```{r}

allData_hex = as.h2o( data_train )
str(allData_hex)
```

## Isolation Forest

Es un método no supervisado para identificar anomalías (outliers) cuando los datos no están etiquetados, es decir, no se conoce la clasificación real (anomalía - no anomalía) de las observaciones.
•	Su funcionamiento está inspirado en el algoritmo de clasificación y regresión Random Forest. 
•	Un modelo Isolation Forest está formado por la combinación de múltiples árboles llamados isolation trees. 
•	El modelo Isolation Forest se obtiene al combinar múltiples isolation tree, cada uno entrenado con una muestra distinta generada por bootstrapping a partir de los datos de originales.


Se aplica la técnica de detección de anomalías h2o.isolationforest

```{r warning=FALSE}
dataModel = h2o.isolationForest( training_frame = allData_hex,
                                 x              = colnames(allData_hex)[-11],
                                 sample_rate    = 0.9,
                                 max_depth      = 100,
                                 ntrees         = 500
                                )
dataModel
```

Se genera la predicción

```{r results='hide', warning=FALSE}
allData_hex_test = as.h2o(data_test)
```

```{r warning=FALSE}
score = h2o.predict( dataModel, allData_hex_test )
result_pred = as.vector( score$predict )

```

```{r}
predicciones_h2o <- h2o.predict(
                      object  = dataModel,
                      newdata = allData_hex
                    )
predicciones <- as.data.frame(predicciones_h2o)
head(predicciones)
```
```{r}
library(ggplot2)
ggplot(data = predicciones, aes(x = mean_length)) +
  geom_histogram(color = "gray40") +
  geom_vline(
    xintercept = quantile(predicciones$mean_length, seq(0, 1, 0.1)),
    color      = "red",
    linetype   = "dashed") +
  labs(
    title = "Distribución de las distancias medias del Isolation Forest",
    subtitle = "Cuantiles marcados en rojo"  ) +
  theme_bw()
```

```{r}
cuantiles <- quantile(x = predicciones$mean_length, probs = seq(0, 1, 0.05))
cuantiles
```

```{r}

datos <- data_train %>%
         bind_cols(predicciones)
head(datos)

```
```{r}
  ggplot(data = datos,
         aes(x = isFradulent, y = mean_length)) +
  geom_jitter(aes(color = isFradulent), width = 0.03, alpha = 0.3) + 
  geom_violin(alpha = 0) +
  geom_boxplot(width = 0.2, outlier.shape = NA, alpha = 0) +
  stat_summary(fun = "mean", colour = "orangered2", size = 3, geom = "point") +
  labs(title = "Distancia promedio en el modelo Isolation Forest",
       x = "clasificación (0 = normal, 1 = anomalía)",
       y = "Distancia promedio") +
  theme_bw() + 
  theme(legend.position = "none")
```

Acorde a la información del dataset, contiene 380 anomalías. 

Se muestra matriz de confusión resultante si se clasifican como anomalías, las 380 observaciones con menor distancia predicha.


```{r}
resultados <- datos %>%
              select(isFradulent, mean_length) %>%
              arrange(mean_length) %>%
              mutate(clasificacion = if_else(row_number() <= 380, "1", "0"))

```


```{r}
mat_confusion <- MLmetrics::ConfusionMatrix(
                    y_pred = resultados$clasificacion,
                    y_true = resultados$isFradulent
                 )
mat_confusion
```


```{r}
a = 310 / 380 
falsos_positivos = 1 - a
falsos_positivos * 100
```


## Técnica de agrupamiento K-means

K-means es un algoritmo de clasificación no supervisada (clusterización) que agrupa objetos en k grupos basándose en sus características. 

El agrupamiento se realiza minimizando la suma de distancias entre cada objeto y el centroide de su grupo o cluster. Se suele usar la distancia cuadrática.

El algoritmo k-means resuelve un problema de optimización, siendo la función a optimizar (minimizar) la suma de las distancias cuadráticas de cada objeto al centroide de su cluster.

```{r}
set.seed(1234)
data_2 <- as.data.frame(lapply(data, scale))

```

```{r}
clusters <- kmeans(data_2,2)
clusplot(data_2,
         clusters$cluster,
         color = TRUE
         )
```
```{r}
clusplot(data_2,
         clusters$cluster,
         color = TRUE,
         col.clus=c(1:2)[unique(clusters$cluster)],
         shade = TRUE,
         labels = 2,
         lines=0, 
         main = "Bivariate Cluster Plot")
```
```{r}
#library()
#plotcluster(data_2, clusters$cluster)
clusters$size
clusters$centers
```

```{r}
data_2$cluster <- clusters$cluster

data_2[1:10,c("cluster","isFradulent","isForeignTransaction","isHighRiskCountry"
              )]
```

```{r}
aggregate(data = data_2,isFradulent ~ cluster,mean)
aggregate(data = data_2,isForeignTransaction ~ cluster,mean)
aggregate(data = data_2,isHighRiskCountry ~ cluster,mean)

```

## Conclusiones
* Se realizó el análisis de los datos del data frame cargando y revisando cada una de las variables observando que se tiene 12 variables con 3075 observaciones.
En el análisis de los datos se observó que se cuenta con 1 variable lógica, 7 numéricas y 4 tipo char.

* Al realizar la correlación de las variables se encuentra alta correlación entre la varible isFradulen con Transaction_amount, Total Number of declines/day, isForeignTransaction, isHighRiskCountry, 6-month_chbk_freq, tambien de la variable  
Transaction_amount con Average Amount/transaction/day, is declined con  Daily_chargeback_avg_amt , 6_month_avg_chbk_amt, 6-month_chbk_freq, de la variable isForeignTransaction con  isHighRiskCountry, entre las más relevantes.

* La detección de anomalías para el conjunto de datos la predicción permite inferir que la información de la data cargada contiene 380 anomalías clasificadas de manera adecuada por el método.

* Se observó que para el ejemplo con K-means no queda tan claro el concepto, por lo que se sugeriría realizarla con otro método para ver si mejora. 
En cuanto a los clusters usados se notó que los datos se situaron a la periferia de los círculos que identifican el grupo, y otro tanto dispersos sobre el círculo mayor, lo que no se deja ver muy claro el concepto, como sí aparece en el ejemplo propuesto por el profesor en las magistrales.


